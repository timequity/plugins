{
  "name": "spark-job",
  "description": "PySpark job template for distributed data processing",
  "stack": {
    "framework": "PySpark",
    "storage": "S3 / Delta Lake",
    "deploy": "EMR / Databricks / Glue"
  },
  "dependencies": {
    "pyspark": "^3.5.0",
    "delta-spark": "^3.0.0",
    "pytest": "^8.0.0"
  },
  "structure": [
    "src/",
    "src/jobs/",
    "src/transformations/",
    "src/utils/",
    "tests/",
    "config/",
    "requirements.txt",
    "setup.py"
  ],
  "features": {
    "patterns": ["Extract", "Transform", "Load"],
    "formats": ["Parquet", "Delta", "CSV", "JSON"],
    "optimizations": ["Partitioning", "Caching", "Broadcast joins"]
  }
}
